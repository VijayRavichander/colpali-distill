{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vijayravichander/Code/ColPali/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"vidore/arxivqa_test_subsampled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds[\"test\"].take(2)\n",
    "\n",
    "# from PIL import Image\n",
    "# from datasets import Dataset, DatasetDict\n",
    "\n",
    "# # Create images\n",
    "# images = [\n",
    "#     Image.new(\"RGB\", (16, 16), color=\"black\"),\n",
    "# ]\n",
    "\n",
    "# # Corresponding queries\n",
    "# queries = [\n",
    "#     \"Is attention really all you need?\",\n",
    "# ]\n",
    "\n",
    "# # Combine images and queries into a list of dictionaries\n",
    "# data = {\"image\": images, \"query\": queries}\n",
    "\n",
    "# ds = Dataset.from_dict(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['query', 'image', 'image_filename', 'options', 'answer', 'page', 'model', 'prompt', 'source'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToPILImage\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy\n",
    "\n",
    "to_pil_image = ToPILImage()\n",
    "\n",
    "def plot_image(img):\n",
    "    # Convert tensor or NumPy array to PIL image if necessary\n",
    "    if isinstance(img, torch.Tensor) or isinstance(img, numpy.ndarray):\n",
    "        img = to_pil_image(img)\n",
    "    \n",
    "    # Display the image using matplotlib\n",
    "    plt.axis(\"off\")  # Turn off axes for cleaner visualization\n",
    "    plt.imshow(img)  # Show the image\n",
    "    plt.show()       # Render the plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple, Union, cast\n",
    "from PIL.Image import Image\n",
    "\n",
    "\n",
    "texts_query: List[Union[str, None]] = []\n",
    "images: List[Image] = []\n",
    "\n",
    "# Parse the examples.\n",
    "for example in ds:\n",
    "    query = example.get(\"query\")\n",
    "    texts_query.append(query)\n",
    "\n",
    "    image = example.get(\"image\")\n",
    "    if image is None:\n",
    "        raise ValueError(\"Image is None - This collator does not support None images yet.\")\n",
    "    images.append(cast(Image, image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collator import VisualRetrieverCollator\n",
    "from colpali_engine.models import ColIdefics3, ColIdefics3Processor\n",
    "\n",
    "processor = ColIdefics3Processor.from_pretrained(\"vidore/colSmol-256M\")\n",
    "vsc = VisualRetrieverCollator(processor, 256)\n",
    "# processor.image_processor.do_image_splitting = False  # For Simplicity\n",
    "\n",
    "preprocessed_inputs = vsc(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_pixel_values\n",
      "torch.Size([2, 13, 3, 512, 512])\n",
      "torch.float32\n",
      "doc_pixel_attention_mask\n",
      "torch.Size([2, 13, 512, 512])\n",
      "torch.float32\n",
      "doc_input_ids\n",
      "torch.Size([2, 870])\n",
      "torch.float32\n",
      "doc_attention_mask\n",
      "torch.Size([2, 870])\n",
      "torch.float32\n",
      "query_input_ids\n",
      "torch.Size([2, 39])\n",
      "torch.float32\n",
      "query_attention_mask\n",
      "torch.Size([2, 39])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "for k, v in preprocessed_inputs.items():\n",
    "    print(k)\n",
    "    print(v.shape)\n",
    "    print(v.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc Input tokens: <|im_start|>User: Describe the image.<fake_token_around_image><row_1_col_1><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><fake_token_around_image><row_1_col_2><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><fake_token_around_image><row_1_col_3><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><fake_token_around_image><row_1_col_4><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image>\n",
      "<fake_token_around_image><row_2_col_1><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><fake_token_around_image><row_2_col_2><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><fake_token_around_image><row_2_col_3><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><fake_token_around_image><row_2_col_4><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image>\n",
      "<fake_token_around_image><row_3_col_1><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><fake_token_around_image><row_3_col_2><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><fake_token_around_image><row_3_col_3><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><fake_token_around_image><row_3_col_4><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image>\n",
      "\n",
      "<fake_token_around_image><global-img><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><fake_token_around_image><end_of_utterance>\n",
      "\n",
      "Query Input tokens: Query: Based on the graph, what is the impact of correcting for fspec not equal to 1 on the surface density trend?<end_of_utterance><end_of_utterance><end_of_utterance><end_of_utterance><end_of_utterance><end_of_utterance><end_of_utterance><end_of_utterance><end_of_utterance><end_of_utterance>\n",
      "<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "doc_input_ids = preprocessed_inputs[\"doc_input_ids\"]\n",
    "doc_input_tokens = processor.tokenizer.decode(doc_input_ids[0])\n",
    "print(\"Doc Input tokens:\", doc_input_tokens)\n",
    "\n",
    "print()\n",
    "query_input_ids = preprocessed_inputs[\"query_input_ids\"]\n",
    "query_input_tokens = processor.tokenizer.decode(query_input_ids[0])\n",
    "print(\"Query Input tokens:\", query_input_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colpali_engine.models import ColIdefics3, ColIdefics3Processor\n",
    "\n",
    "model = ColIdefics3.from_pretrained(\n",
    "        \"vidore/colSmol-256M\",\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"mps\",\n",
    "        attn_implementation= None\n",
    "    ).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Set device to MPS\n",
    "device = torch.device('mps')\n",
    "\n",
    "\n",
    "# Move input tensors to MPS device\n",
    "query_input_ids = preprocessed_inputs[\"query_input_ids\"].to(device)\n",
    "query_attention_mask = preprocessed_inputs[\"query_attention_mask\"].to(device)\n",
    "\n",
    "# Perform inference with the model\n",
    "with torch.no_grad():\n",
    "    query_outputs = model(input_ids=query_input_ids, attention_mask=query_attention_mask)\n",
    "    # Feed only kwargs with 'doc_' prefix\n",
    "    doc_outputs = model(**{k[4:]: v.to(torch.int32).to(device) for k, v in preprocessed_inputs.items() if k.startswith(\"doc\")})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 39, 128])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 870, 128])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24.5469, 14.6641],\n",
      "        [15.3516, 19.5938]], device='mps:0', dtype=torch.float16)\n",
      "tensor([24.5469, 19.5938], device='mps:0', dtype=torch.float16)\n",
      "tensor([15.3516, 14.6641], device='mps:0', dtype=torch.float16)\n",
      "tensor(0.0037, device='mps:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "scores = torch.einsum(\"bnd,csd->bcns\", query_outputs, doc_outputs).max(dim = 3)[0].sum(dim = -1)\n",
    "\n",
    "print(scores)\n",
    "\n",
    "pos_scores = torch.diag(scores)\n",
    "\n",
    "# # ColPali Implementation of Neg Scores\n",
    "# negative_scores = scores - torch.eye(scores.shape[0], device = scores.device) * 1e6\n",
    "# negative_scores = negative_scores.max(dim = 0)[0]\n",
    "\n",
    "# loss = F.softplus(negative_scores - pos_scores).mean()\n",
    "\n",
    "# Self\n",
    "mask = torch.eye(scores.shape[0], device = scores.device).bool()\n",
    "s_masked = scores.masked_fill(mask, float('-inf'))\n",
    "neg_scores = s_masked.max(dim = 0)[0]\n",
    "\n",
    "loss = F.softplus(neg_scores - pos_scores).mean()\n",
    "\n",
    "print(pos_scores)\n",
    "print(neg_scores)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, PeftModel, get_peft_model\n",
    "\n",
    "model=ColIdefics3.from_pretrained(\"vidore/ColSmolVLM-Instruct-256M-base\", torch_dtype=torch.float16, attn_implementation=\"eager\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ColIdefics3(\n",
       "  (model): Idefics3Model(\n",
       "    (vision_model): Idefics3VisionTransformer(\n",
       "      (embeddings): Idefics3VisionEmbeddings(\n",
       "        (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), padding=valid)\n",
       "        (position_embedding): Embedding(1024, 768)\n",
       "      )\n",
       "      (encoder): Idefics3Encoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x Idefics3EncoderLayer(\n",
       "            (self_attn): Idefics3VisionAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Idefics3VisionMLP(\n",
       "              (activation_fn): PytorchGELUTanh()\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (connector): Idefics3Connector(\n",
       "      (modality_projection): Idefics3SimpleMLP(\n",
       "        (proj): Linear(in_features=12288, out_features=576, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (text_model): LlamaModel(\n",
       "      (embed_tokens): Embedding(49280, 576, padding_idx=2)\n",
       "      (layers): ModuleList(\n",
       "        (0-29): 30 x LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "            (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "            (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "            (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "            (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "            (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "          (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=576, out_features=128, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model\n",
      "model.vision_model\n",
      "model.vision_model.embeddings\n",
      "model.vision_model.embeddings.patch_embedding\n",
      "model.vision_model.embeddings.position_embedding\n",
      "model.vision_model.encoder\n",
      "model.vision_model.encoder.layers\n",
      "model.vision_model.encoder.layers.0\n",
      "model.vision_model.encoder.layers.0.self_attn\n",
      "model.vision_model.encoder.layers.0.self_attn.k_proj\n",
      "model.vision_model.encoder.layers.0.self_attn.v_proj\n",
      "model.vision_model.encoder.layers.0.self_attn.q_proj\n",
      "model.vision_model.encoder.layers.0.self_attn.out_proj\n",
      "model.vision_model.encoder.layers.0.layer_norm1\n",
      "model.vision_model.encoder.layers.0.mlp\n",
      "model.vision_model.encoder.layers.0.mlp.activation_fn\n",
      "model.vision_model.encoder.layers.0.mlp.fc1\n",
      "model.vision_model.encoder.layers.0.mlp.fc2\n",
      "model.vision_model.encoder.layers.0.layer_norm2\n",
      "model.vision_model.encoder.layers.1\n",
      "model.vision_model.encoder.layers.1.self_attn\n",
      "model.vision_model.encoder.layers.1.self_attn.k_proj\n",
      "model.vision_model.encoder.layers.1.self_attn.v_proj\n",
      "model.vision_model.encoder.layers.1.self_attn.q_proj\n",
      "model.vision_model.encoder.layers.1.self_attn.out_proj\n",
      "model.vision_model.encoder.layers.1.layer_norm1\n",
      "model.vision_model.encoder.layers.1.mlp\n",
      "model.vision_model.encoder.layers.1.mlp.activation_fn\n",
      "model.vision_model.encoder.layers.1.mlp.fc1\n",
      "model.vision_model.encoder.layers.1.mlp.fc2\n",
      "model.vision_model.encoder.layers.1.layer_norm2\n",
      "model.vision_model.encoder.layers.2\n",
      "model.vision_model.encoder.layers.2.self_attn\n",
      "model.vision_model.encoder.layers.2.self_attn.k_proj\n",
      "model.vision_model.encoder.layers.2.self_attn.v_proj\n",
      "model.vision_model.encoder.layers.2.self_attn.q_proj\n",
      "model.vision_model.encoder.layers.2.self_attn.out_proj\n",
      "model.vision_model.encoder.layers.2.layer_norm1\n",
      "model.vision_model.encoder.layers.2.mlp\n",
      "model.vision_model.encoder.layers.2.mlp.activation_fn\n",
      "model.vision_model.encoder.layers.2.mlp.fc1\n",
      "model.vision_model.encoder.layers.2.mlp.fc2\n",
      "model.vision_model.encoder.layers.2.layer_norm2\n",
      "model.vision_model.encoder.layers.3\n",
      "model.vision_model.encoder.layers.3.self_attn\n",
      "model.vision_model.encoder.layers.3.self_attn.k_proj\n",
      "model.vision_model.encoder.layers.3.self_attn.v_proj\n",
      "model.vision_model.encoder.layers.3.self_attn.q_proj\n",
      "model.vision_model.encoder.layers.3.self_attn.out_proj\n",
      "model.vision_model.encoder.layers.3.layer_norm1\n",
      "model.vision_model.encoder.layers.3.mlp\n",
      "model.vision_model.encoder.layers.3.mlp.activation_fn\n",
      "model.vision_model.encoder.layers.3.mlp.fc1\n",
      "model.vision_model.encoder.layers.3.mlp.fc2\n",
      "model.vision_model.encoder.layers.3.layer_norm2\n",
      "model.vision_model.encoder.layers.4\n",
      "model.vision_model.encoder.layers.4.self_attn\n",
      "model.vision_model.encoder.layers.4.self_attn.k_proj\n",
      "model.vision_model.encoder.layers.4.self_attn.v_proj\n",
      "model.vision_model.encoder.layers.4.self_attn.q_proj\n",
      "model.vision_model.encoder.layers.4.self_attn.out_proj\n",
      "model.vision_model.encoder.layers.4.layer_norm1\n",
      "model.vision_model.encoder.layers.4.mlp\n",
      "model.vision_model.encoder.layers.4.mlp.activation_fn\n",
      "model.vision_model.encoder.layers.4.mlp.fc1\n",
      "model.vision_model.encoder.layers.4.mlp.fc2\n",
      "model.vision_model.encoder.layers.4.layer_norm2\n",
      "model.vision_model.encoder.layers.5\n",
      "model.vision_model.encoder.layers.5.self_attn\n",
      "model.vision_model.encoder.layers.5.self_attn.k_proj\n",
      "model.vision_model.encoder.layers.5.self_attn.v_proj\n",
      "model.vision_model.encoder.layers.5.self_attn.q_proj\n",
      "model.vision_model.encoder.layers.5.self_attn.out_proj\n",
      "model.vision_model.encoder.layers.5.layer_norm1\n",
      "model.vision_model.encoder.layers.5.mlp\n",
      "model.vision_model.encoder.layers.5.mlp.activation_fn\n",
      "model.vision_model.encoder.layers.5.mlp.fc1\n",
      "model.vision_model.encoder.layers.5.mlp.fc2\n",
      "model.vision_model.encoder.layers.5.layer_norm2\n",
      "model.vision_model.encoder.layers.6\n",
      "model.vision_model.encoder.layers.6.self_attn\n",
      "model.vision_model.encoder.layers.6.self_attn.k_proj\n",
      "model.vision_model.encoder.layers.6.self_attn.v_proj\n",
      "model.vision_model.encoder.layers.6.self_attn.q_proj\n",
      "model.vision_model.encoder.layers.6.self_attn.out_proj\n",
      "model.vision_model.encoder.layers.6.layer_norm1\n",
      "model.vision_model.encoder.layers.6.mlp\n",
      "model.vision_model.encoder.layers.6.mlp.activation_fn\n",
      "model.vision_model.encoder.layers.6.mlp.fc1\n",
      "model.vision_model.encoder.layers.6.mlp.fc2\n",
      "model.vision_model.encoder.layers.6.layer_norm2\n",
      "model.vision_model.encoder.layers.7\n",
      "model.vision_model.encoder.layers.7.self_attn\n",
      "model.vision_model.encoder.layers.7.self_attn.k_proj\n",
      "model.vision_model.encoder.layers.7.self_attn.v_proj\n",
      "model.vision_model.encoder.layers.7.self_attn.q_proj\n",
      "model.vision_model.encoder.layers.7.self_attn.out_proj\n",
      "model.vision_model.encoder.layers.7.layer_norm1\n",
      "model.vision_model.encoder.layers.7.mlp\n",
      "model.vision_model.encoder.layers.7.mlp.activation_fn\n",
      "model.vision_model.encoder.layers.7.mlp.fc1\n",
      "model.vision_model.encoder.layers.7.mlp.fc2\n",
      "model.vision_model.encoder.layers.7.layer_norm2\n",
      "model.vision_model.encoder.layers.8\n",
      "model.vision_model.encoder.layers.8.self_attn\n",
      "model.vision_model.encoder.layers.8.self_attn.k_proj\n",
      "model.vision_model.encoder.layers.8.self_attn.v_proj\n",
      "model.vision_model.encoder.layers.8.self_attn.q_proj\n",
      "model.vision_model.encoder.layers.8.self_attn.out_proj\n",
      "model.vision_model.encoder.layers.8.layer_norm1\n",
      "model.vision_model.encoder.layers.8.mlp\n",
      "model.vision_model.encoder.layers.8.mlp.activation_fn\n",
      "model.vision_model.encoder.layers.8.mlp.fc1\n",
      "model.vision_model.encoder.layers.8.mlp.fc2\n",
      "model.vision_model.encoder.layers.8.layer_norm2\n",
      "model.vision_model.encoder.layers.9\n",
      "model.vision_model.encoder.layers.9.self_attn\n",
      "model.vision_model.encoder.layers.9.self_attn.k_proj\n",
      "model.vision_model.encoder.layers.9.self_attn.v_proj\n",
      "model.vision_model.encoder.layers.9.self_attn.q_proj\n",
      "model.vision_model.encoder.layers.9.self_attn.out_proj\n",
      "model.vision_model.encoder.layers.9.layer_norm1\n",
      "model.vision_model.encoder.layers.9.mlp\n",
      "model.vision_model.encoder.layers.9.mlp.activation_fn\n",
      "model.vision_model.encoder.layers.9.mlp.fc1\n",
      "model.vision_model.encoder.layers.9.mlp.fc2\n",
      "model.vision_model.encoder.layers.9.layer_norm2\n",
      "model.vision_model.encoder.layers.10\n",
      "model.vision_model.encoder.layers.10.self_attn\n",
      "model.vision_model.encoder.layers.10.self_attn.k_proj\n",
      "model.vision_model.encoder.layers.10.self_attn.v_proj\n",
      "model.vision_model.encoder.layers.10.self_attn.q_proj\n",
      "model.vision_model.encoder.layers.10.self_attn.out_proj\n",
      "model.vision_model.encoder.layers.10.layer_norm1\n",
      "model.vision_model.encoder.layers.10.mlp\n",
      "model.vision_model.encoder.layers.10.mlp.activation_fn\n",
      "model.vision_model.encoder.layers.10.mlp.fc1\n",
      "model.vision_model.encoder.layers.10.mlp.fc2\n",
      "model.vision_model.encoder.layers.10.layer_norm2\n",
      "model.vision_model.encoder.layers.11\n",
      "model.vision_model.encoder.layers.11.self_attn\n",
      "model.vision_model.encoder.layers.11.self_attn.k_proj\n",
      "model.vision_model.encoder.layers.11.self_attn.v_proj\n",
      "model.vision_model.encoder.layers.11.self_attn.q_proj\n",
      "model.vision_model.encoder.layers.11.self_attn.out_proj\n",
      "model.vision_model.encoder.layers.11.layer_norm1\n",
      "model.vision_model.encoder.layers.11.mlp\n",
      "model.vision_model.encoder.layers.11.mlp.activation_fn\n",
      "model.vision_model.encoder.layers.11.mlp.fc1\n",
      "model.vision_model.encoder.layers.11.mlp.fc2\n",
      "model.vision_model.encoder.layers.11.layer_norm2\n",
      "model.vision_model.post_layernorm\n",
      "model.connector\n",
      "model.connector.modality_projection\n",
      "model.connector.modality_projection.proj\n",
      "model.text_model\n",
      "model.text_model.embed_tokens\n",
      "model.text_model.layers\n",
      "model.text_model.layers.0\n",
      "model.text_model.layers.0.self_attn\n",
      "model.text_model.layers.0.self_attn.q_proj\n",
      "model.text_model.layers.0.self_attn.k_proj\n",
      "model.text_model.layers.0.self_attn.v_proj\n",
      "model.text_model.layers.0.self_attn.o_proj\n",
      "model.text_model.layers.0.mlp\n",
      "model.text_model.layers.0.mlp.gate_proj\n",
      "model.text_model.layers.0.mlp.up_proj\n",
      "model.text_model.layers.0.mlp.down_proj\n",
      "model.text_model.layers.0.mlp.act_fn\n",
      "model.text_model.layers.0.input_layernorm\n",
      "model.text_model.layers.0.post_attention_layernorm\n",
      "model.text_model.layers.1\n",
      "model.text_model.layers.1.self_attn\n",
      "model.text_model.layers.1.self_attn.q_proj\n",
      "model.text_model.layers.1.self_attn.k_proj\n",
      "model.text_model.layers.1.self_attn.v_proj\n",
      "model.text_model.layers.1.self_attn.o_proj\n",
      "model.text_model.layers.1.mlp\n",
      "model.text_model.layers.1.mlp.gate_proj\n",
      "model.text_model.layers.1.mlp.up_proj\n",
      "model.text_model.layers.1.mlp.down_proj\n",
      "model.text_model.layers.1.mlp.act_fn\n",
      "model.text_model.layers.1.input_layernorm\n",
      "model.text_model.layers.1.post_attention_layernorm\n",
      "model.text_model.layers.2\n",
      "model.text_model.layers.2.self_attn\n",
      "model.text_model.layers.2.self_attn.q_proj\n",
      "model.text_model.layers.2.self_attn.k_proj\n",
      "model.text_model.layers.2.self_attn.v_proj\n",
      "model.text_model.layers.2.self_attn.o_proj\n",
      "model.text_model.layers.2.mlp\n",
      "model.text_model.layers.2.mlp.gate_proj\n",
      "model.text_model.layers.2.mlp.up_proj\n",
      "model.text_model.layers.2.mlp.down_proj\n",
      "model.text_model.layers.2.mlp.act_fn\n",
      "model.text_model.layers.2.input_layernorm\n",
      "model.text_model.layers.2.post_attention_layernorm\n",
      "model.text_model.layers.3\n",
      "model.text_model.layers.3.self_attn\n",
      "model.text_model.layers.3.self_attn.q_proj\n",
      "model.text_model.layers.3.self_attn.k_proj\n",
      "model.text_model.layers.3.self_attn.v_proj\n",
      "model.text_model.layers.3.self_attn.o_proj\n",
      "model.text_model.layers.3.mlp\n",
      "model.text_model.layers.3.mlp.gate_proj\n",
      "model.text_model.layers.3.mlp.up_proj\n",
      "model.text_model.layers.3.mlp.down_proj\n",
      "model.text_model.layers.3.mlp.act_fn\n",
      "model.text_model.layers.3.input_layernorm\n",
      "model.text_model.layers.3.post_attention_layernorm\n",
      "model.text_model.layers.4\n",
      "model.text_model.layers.4.self_attn\n",
      "model.text_model.layers.4.self_attn.q_proj\n",
      "model.text_model.layers.4.self_attn.k_proj\n",
      "model.text_model.layers.4.self_attn.v_proj\n",
      "model.text_model.layers.4.self_attn.o_proj\n",
      "model.text_model.layers.4.mlp\n",
      "model.text_model.layers.4.mlp.gate_proj\n",
      "model.text_model.layers.4.mlp.up_proj\n",
      "model.text_model.layers.4.mlp.down_proj\n",
      "model.text_model.layers.4.mlp.act_fn\n",
      "model.text_model.layers.4.input_layernorm\n",
      "model.text_model.layers.4.post_attention_layernorm\n",
      "model.text_model.layers.5\n",
      "model.text_model.layers.5.self_attn\n",
      "model.text_model.layers.5.self_attn.q_proj\n",
      "model.text_model.layers.5.self_attn.k_proj\n",
      "model.text_model.layers.5.self_attn.v_proj\n",
      "model.text_model.layers.5.self_attn.o_proj\n",
      "model.text_model.layers.5.mlp\n",
      "model.text_model.layers.5.mlp.gate_proj\n",
      "model.text_model.layers.5.mlp.up_proj\n",
      "model.text_model.layers.5.mlp.down_proj\n",
      "model.text_model.layers.5.mlp.act_fn\n",
      "model.text_model.layers.5.input_layernorm\n",
      "model.text_model.layers.5.post_attention_layernorm\n",
      "model.text_model.layers.6\n",
      "model.text_model.layers.6.self_attn\n",
      "model.text_model.layers.6.self_attn.q_proj\n",
      "model.text_model.layers.6.self_attn.k_proj\n",
      "model.text_model.layers.6.self_attn.v_proj\n",
      "model.text_model.layers.6.self_attn.o_proj\n",
      "model.text_model.layers.6.mlp\n",
      "model.text_model.layers.6.mlp.gate_proj\n",
      "model.text_model.layers.6.mlp.up_proj\n",
      "model.text_model.layers.6.mlp.down_proj\n",
      "model.text_model.layers.6.mlp.act_fn\n",
      "model.text_model.layers.6.input_layernorm\n",
      "model.text_model.layers.6.post_attention_layernorm\n",
      "model.text_model.layers.7\n",
      "model.text_model.layers.7.self_attn\n",
      "model.text_model.layers.7.self_attn.q_proj\n",
      "model.text_model.layers.7.self_attn.k_proj\n",
      "model.text_model.layers.7.self_attn.v_proj\n",
      "model.text_model.layers.7.self_attn.o_proj\n",
      "model.text_model.layers.7.mlp\n",
      "model.text_model.layers.7.mlp.gate_proj\n",
      "model.text_model.layers.7.mlp.up_proj\n",
      "model.text_model.layers.7.mlp.down_proj\n",
      "model.text_model.layers.7.mlp.act_fn\n",
      "model.text_model.layers.7.input_layernorm\n",
      "model.text_model.layers.7.post_attention_layernorm\n",
      "model.text_model.layers.8\n",
      "model.text_model.layers.8.self_attn\n",
      "model.text_model.layers.8.self_attn.q_proj\n",
      "model.text_model.layers.8.self_attn.k_proj\n",
      "model.text_model.layers.8.self_attn.v_proj\n",
      "model.text_model.layers.8.self_attn.o_proj\n",
      "model.text_model.layers.8.mlp\n",
      "model.text_model.layers.8.mlp.gate_proj\n",
      "model.text_model.layers.8.mlp.up_proj\n",
      "model.text_model.layers.8.mlp.down_proj\n",
      "model.text_model.layers.8.mlp.act_fn\n",
      "model.text_model.layers.8.input_layernorm\n",
      "model.text_model.layers.8.post_attention_layernorm\n",
      "model.text_model.layers.9\n",
      "model.text_model.layers.9.self_attn\n",
      "model.text_model.layers.9.self_attn.q_proj\n",
      "model.text_model.layers.9.self_attn.k_proj\n",
      "model.text_model.layers.9.self_attn.v_proj\n",
      "model.text_model.layers.9.self_attn.o_proj\n",
      "model.text_model.layers.9.mlp\n",
      "model.text_model.layers.9.mlp.gate_proj\n",
      "model.text_model.layers.9.mlp.up_proj\n",
      "model.text_model.layers.9.mlp.down_proj\n",
      "model.text_model.layers.9.mlp.act_fn\n",
      "model.text_model.layers.9.input_layernorm\n",
      "model.text_model.layers.9.post_attention_layernorm\n",
      "model.text_model.layers.10\n",
      "model.text_model.layers.10.self_attn\n",
      "model.text_model.layers.10.self_attn.q_proj\n",
      "model.text_model.layers.10.self_attn.k_proj\n",
      "model.text_model.layers.10.self_attn.v_proj\n",
      "model.text_model.layers.10.self_attn.o_proj\n",
      "model.text_model.layers.10.mlp\n",
      "model.text_model.layers.10.mlp.gate_proj\n",
      "model.text_model.layers.10.mlp.up_proj\n",
      "model.text_model.layers.10.mlp.down_proj\n",
      "model.text_model.layers.10.mlp.act_fn\n",
      "model.text_model.layers.10.input_layernorm\n",
      "model.text_model.layers.10.post_attention_layernorm\n",
      "model.text_model.layers.11\n",
      "model.text_model.layers.11.self_attn\n",
      "model.text_model.layers.11.self_attn.q_proj\n",
      "model.text_model.layers.11.self_attn.k_proj\n",
      "model.text_model.layers.11.self_attn.v_proj\n",
      "model.text_model.layers.11.self_attn.o_proj\n",
      "model.text_model.layers.11.mlp\n",
      "model.text_model.layers.11.mlp.gate_proj\n",
      "model.text_model.layers.11.mlp.up_proj\n",
      "model.text_model.layers.11.mlp.down_proj\n",
      "model.text_model.layers.11.mlp.act_fn\n",
      "model.text_model.layers.11.input_layernorm\n",
      "model.text_model.layers.11.post_attention_layernorm\n",
      "model.text_model.layers.12\n",
      "model.text_model.layers.12.self_attn\n",
      "model.text_model.layers.12.self_attn.q_proj\n",
      "model.text_model.layers.12.self_attn.k_proj\n",
      "model.text_model.layers.12.self_attn.v_proj\n",
      "model.text_model.layers.12.self_attn.o_proj\n",
      "model.text_model.layers.12.mlp\n",
      "model.text_model.layers.12.mlp.gate_proj\n",
      "model.text_model.layers.12.mlp.up_proj\n",
      "model.text_model.layers.12.mlp.down_proj\n",
      "model.text_model.layers.12.mlp.act_fn\n",
      "model.text_model.layers.12.input_layernorm\n",
      "model.text_model.layers.12.post_attention_layernorm\n",
      "model.text_model.layers.13\n",
      "model.text_model.layers.13.self_attn\n",
      "model.text_model.layers.13.self_attn.q_proj\n",
      "model.text_model.layers.13.self_attn.k_proj\n",
      "model.text_model.layers.13.self_attn.v_proj\n",
      "model.text_model.layers.13.self_attn.o_proj\n",
      "model.text_model.layers.13.mlp\n",
      "model.text_model.layers.13.mlp.gate_proj\n",
      "model.text_model.layers.13.mlp.up_proj\n",
      "model.text_model.layers.13.mlp.down_proj\n",
      "model.text_model.layers.13.mlp.act_fn\n",
      "model.text_model.layers.13.input_layernorm\n",
      "model.text_model.layers.13.post_attention_layernorm\n",
      "model.text_model.layers.14\n",
      "model.text_model.layers.14.self_attn\n",
      "model.text_model.layers.14.self_attn.q_proj\n",
      "model.text_model.layers.14.self_attn.k_proj\n",
      "model.text_model.layers.14.self_attn.v_proj\n",
      "model.text_model.layers.14.self_attn.o_proj\n",
      "model.text_model.layers.14.mlp\n",
      "model.text_model.layers.14.mlp.gate_proj\n",
      "model.text_model.layers.14.mlp.up_proj\n",
      "model.text_model.layers.14.mlp.down_proj\n",
      "model.text_model.layers.14.mlp.act_fn\n",
      "model.text_model.layers.14.input_layernorm\n",
      "model.text_model.layers.14.post_attention_layernorm\n",
      "model.text_model.layers.15\n",
      "model.text_model.layers.15.self_attn\n",
      "model.text_model.layers.15.self_attn.q_proj\n",
      "model.text_model.layers.15.self_attn.k_proj\n",
      "model.text_model.layers.15.self_attn.v_proj\n",
      "model.text_model.layers.15.self_attn.o_proj\n",
      "model.text_model.layers.15.mlp\n",
      "model.text_model.layers.15.mlp.gate_proj\n",
      "model.text_model.layers.15.mlp.up_proj\n",
      "model.text_model.layers.15.mlp.down_proj\n",
      "model.text_model.layers.15.mlp.act_fn\n",
      "model.text_model.layers.15.input_layernorm\n",
      "model.text_model.layers.15.post_attention_layernorm\n",
      "model.text_model.layers.16\n",
      "model.text_model.layers.16.self_attn\n",
      "model.text_model.layers.16.self_attn.q_proj\n",
      "model.text_model.layers.16.self_attn.k_proj\n",
      "model.text_model.layers.16.self_attn.v_proj\n",
      "model.text_model.layers.16.self_attn.o_proj\n",
      "model.text_model.layers.16.mlp\n",
      "model.text_model.layers.16.mlp.gate_proj\n",
      "model.text_model.layers.16.mlp.up_proj\n",
      "model.text_model.layers.16.mlp.down_proj\n",
      "model.text_model.layers.16.mlp.act_fn\n",
      "model.text_model.layers.16.input_layernorm\n",
      "model.text_model.layers.16.post_attention_layernorm\n",
      "model.text_model.layers.17\n",
      "model.text_model.layers.17.self_attn\n",
      "model.text_model.layers.17.self_attn.q_proj\n",
      "model.text_model.layers.17.self_attn.k_proj\n",
      "model.text_model.layers.17.self_attn.v_proj\n",
      "model.text_model.layers.17.self_attn.o_proj\n",
      "model.text_model.layers.17.mlp\n",
      "model.text_model.layers.17.mlp.gate_proj\n",
      "model.text_model.layers.17.mlp.up_proj\n",
      "model.text_model.layers.17.mlp.down_proj\n",
      "model.text_model.layers.17.mlp.act_fn\n",
      "model.text_model.layers.17.input_layernorm\n",
      "model.text_model.layers.17.post_attention_layernorm\n",
      "model.text_model.layers.18\n",
      "model.text_model.layers.18.self_attn\n",
      "model.text_model.layers.18.self_attn.q_proj\n",
      "model.text_model.layers.18.self_attn.k_proj\n",
      "model.text_model.layers.18.self_attn.v_proj\n",
      "model.text_model.layers.18.self_attn.o_proj\n",
      "model.text_model.layers.18.mlp\n",
      "model.text_model.layers.18.mlp.gate_proj\n",
      "model.text_model.layers.18.mlp.up_proj\n",
      "model.text_model.layers.18.mlp.down_proj\n",
      "model.text_model.layers.18.mlp.act_fn\n",
      "model.text_model.layers.18.input_layernorm\n",
      "model.text_model.layers.18.post_attention_layernorm\n",
      "model.text_model.layers.19\n",
      "model.text_model.layers.19.self_attn\n",
      "model.text_model.layers.19.self_attn.q_proj\n",
      "model.text_model.layers.19.self_attn.k_proj\n",
      "model.text_model.layers.19.self_attn.v_proj\n",
      "model.text_model.layers.19.self_attn.o_proj\n",
      "model.text_model.layers.19.mlp\n",
      "model.text_model.layers.19.mlp.gate_proj\n",
      "model.text_model.layers.19.mlp.up_proj\n",
      "model.text_model.layers.19.mlp.down_proj\n",
      "model.text_model.layers.19.mlp.act_fn\n",
      "model.text_model.layers.19.input_layernorm\n",
      "model.text_model.layers.19.post_attention_layernorm\n",
      "model.text_model.layers.20\n",
      "model.text_model.layers.20.self_attn\n",
      "model.text_model.layers.20.self_attn.q_proj\n",
      "model.text_model.layers.20.self_attn.k_proj\n",
      "model.text_model.layers.20.self_attn.v_proj\n",
      "model.text_model.layers.20.self_attn.o_proj\n",
      "model.text_model.layers.20.mlp\n",
      "model.text_model.layers.20.mlp.gate_proj\n",
      "model.text_model.layers.20.mlp.up_proj\n",
      "model.text_model.layers.20.mlp.down_proj\n",
      "model.text_model.layers.20.mlp.act_fn\n",
      "model.text_model.layers.20.input_layernorm\n",
      "model.text_model.layers.20.post_attention_layernorm\n",
      "model.text_model.layers.21\n",
      "model.text_model.layers.21.self_attn\n",
      "model.text_model.layers.21.self_attn.q_proj\n",
      "model.text_model.layers.21.self_attn.k_proj\n",
      "model.text_model.layers.21.self_attn.v_proj\n",
      "model.text_model.layers.21.self_attn.o_proj\n",
      "model.text_model.layers.21.mlp\n",
      "model.text_model.layers.21.mlp.gate_proj\n",
      "model.text_model.layers.21.mlp.up_proj\n",
      "model.text_model.layers.21.mlp.down_proj\n",
      "model.text_model.layers.21.mlp.act_fn\n",
      "model.text_model.layers.21.input_layernorm\n",
      "model.text_model.layers.21.post_attention_layernorm\n",
      "model.text_model.layers.22\n",
      "model.text_model.layers.22.self_attn\n",
      "model.text_model.layers.22.self_attn.q_proj\n",
      "model.text_model.layers.22.self_attn.k_proj\n",
      "model.text_model.layers.22.self_attn.v_proj\n",
      "model.text_model.layers.22.self_attn.o_proj\n",
      "model.text_model.layers.22.mlp\n",
      "model.text_model.layers.22.mlp.gate_proj\n",
      "model.text_model.layers.22.mlp.up_proj\n",
      "model.text_model.layers.22.mlp.down_proj\n",
      "model.text_model.layers.22.mlp.act_fn\n",
      "model.text_model.layers.22.input_layernorm\n",
      "model.text_model.layers.22.post_attention_layernorm\n",
      "model.text_model.layers.23\n",
      "model.text_model.layers.23.self_attn\n",
      "model.text_model.layers.23.self_attn.q_proj\n",
      "model.text_model.layers.23.self_attn.k_proj\n",
      "model.text_model.layers.23.self_attn.v_proj\n",
      "model.text_model.layers.23.self_attn.o_proj\n",
      "model.text_model.layers.23.mlp\n",
      "model.text_model.layers.23.mlp.gate_proj\n",
      "model.text_model.layers.23.mlp.up_proj\n",
      "model.text_model.layers.23.mlp.down_proj\n",
      "model.text_model.layers.23.mlp.act_fn\n",
      "model.text_model.layers.23.input_layernorm\n",
      "model.text_model.layers.23.post_attention_layernorm\n",
      "model.text_model.layers.24\n",
      "model.text_model.layers.24.self_attn\n",
      "model.text_model.layers.24.self_attn.q_proj\n",
      "model.text_model.layers.24.self_attn.k_proj\n",
      "model.text_model.layers.24.self_attn.v_proj\n",
      "model.text_model.layers.24.self_attn.o_proj\n",
      "model.text_model.layers.24.mlp\n",
      "model.text_model.layers.24.mlp.gate_proj\n",
      "model.text_model.layers.24.mlp.up_proj\n",
      "model.text_model.layers.24.mlp.down_proj\n",
      "model.text_model.layers.24.mlp.act_fn\n",
      "model.text_model.layers.24.input_layernorm\n",
      "model.text_model.layers.24.post_attention_layernorm\n",
      "model.text_model.layers.25\n",
      "model.text_model.layers.25.self_attn\n",
      "model.text_model.layers.25.self_attn.q_proj\n",
      "model.text_model.layers.25.self_attn.k_proj\n",
      "model.text_model.layers.25.self_attn.v_proj\n",
      "model.text_model.layers.25.self_attn.o_proj\n",
      "model.text_model.layers.25.mlp\n",
      "model.text_model.layers.25.mlp.gate_proj\n",
      "model.text_model.layers.25.mlp.up_proj\n",
      "model.text_model.layers.25.mlp.down_proj\n",
      "model.text_model.layers.25.mlp.act_fn\n",
      "model.text_model.layers.25.input_layernorm\n",
      "model.text_model.layers.25.post_attention_layernorm\n",
      "model.text_model.layers.26\n",
      "model.text_model.layers.26.self_attn\n",
      "model.text_model.layers.26.self_attn.q_proj\n",
      "model.text_model.layers.26.self_attn.k_proj\n",
      "model.text_model.layers.26.self_attn.v_proj\n",
      "model.text_model.layers.26.self_attn.o_proj\n",
      "model.text_model.layers.26.mlp\n",
      "model.text_model.layers.26.mlp.gate_proj\n",
      "model.text_model.layers.26.mlp.up_proj\n",
      "model.text_model.layers.26.mlp.down_proj\n",
      "model.text_model.layers.26.mlp.act_fn\n",
      "model.text_model.layers.26.input_layernorm\n",
      "model.text_model.layers.26.post_attention_layernorm\n",
      "model.text_model.layers.27\n",
      "model.text_model.layers.27.self_attn\n",
      "model.text_model.layers.27.self_attn.q_proj\n",
      "model.text_model.layers.27.self_attn.k_proj\n",
      "model.text_model.layers.27.self_attn.v_proj\n",
      "model.text_model.layers.27.self_attn.o_proj\n",
      "model.text_model.layers.27.mlp\n",
      "model.text_model.layers.27.mlp.gate_proj\n",
      "model.text_model.layers.27.mlp.up_proj\n",
      "model.text_model.layers.27.mlp.down_proj\n",
      "model.text_model.layers.27.mlp.act_fn\n",
      "model.text_model.layers.27.input_layernorm\n",
      "model.text_model.layers.27.post_attention_layernorm\n",
      "model.text_model.layers.28\n",
      "model.text_model.layers.28.self_attn\n",
      "model.text_model.layers.28.self_attn.q_proj\n",
      "model.text_model.layers.28.self_attn.k_proj\n",
      "model.text_model.layers.28.self_attn.v_proj\n",
      "model.text_model.layers.28.self_attn.o_proj\n",
      "model.text_model.layers.28.mlp\n",
      "model.text_model.layers.28.mlp.gate_proj\n",
      "model.text_model.layers.28.mlp.up_proj\n",
      "model.text_model.layers.28.mlp.down_proj\n",
      "model.text_model.layers.28.mlp.act_fn\n",
      "model.text_model.layers.28.input_layernorm\n",
      "model.text_model.layers.28.post_attention_layernorm\n",
      "model.text_model.layers.29\n",
      "model.text_model.layers.29.self_attn\n",
      "model.text_model.layers.29.self_attn.q_proj\n",
      "model.text_model.layers.29.self_attn.k_proj\n",
      "model.text_model.layers.29.self_attn.v_proj\n",
      "model.text_model.layers.29.self_attn.o_proj\n",
      "model.text_model.layers.29.mlp\n",
      "model.text_model.layers.29.mlp.gate_proj\n",
      "model.text_model.layers.29.mlp.up_proj\n",
      "model.text_model.layers.29.mlp.down_proj\n",
      "model.text_model.layers.29.mlp.act_fn\n",
      "model.text_model.layers.29.input_layernorm\n",
      "model.text_model.layers.29.post_attention_layernorm\n",
      "model.text_model.norm\n",
      "model.text_model.rotary_emb\n",
      "linear\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    init_lora_weights=\"gaussian\",\n",
    "    bias=\"none\",\n",
    "    task_type=\"FEATURE_EXTRACTION\",\n",
    "    target_modules=r\"(.*(model.text_model).*(down_proj|gate_proj|up_proj|k_proj|q_proj|v_proj|o_proj).*$|.*(linear).*$)\", \n",
    "    modules_to_save = [\"linear\"]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForFeatureExtraction(\n",
       "  (base_model): LoraModel(\n",
       "    (model): ColIdefics3(\n",
       "      (model): Idefics3Model(\n",
       "        (vision_model): Idefics3VisionTransformer(\n",
       "          (embeddings): Idefics3VisionEmbeddings(\n",
       "            (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), padding=valid)\n",
       "            (position_embedding): Embedding(1024, 768)\n",
       "          )\n",
       "          (encoder): Idefics3Encoder(\n",
       "            (layers): ModuleList(\n",
       "              (0-11): 12 x Idefics3EncoderLayer(\n",
       "                (self_attn): Idefics3VisionAttention(\n",
       "                  (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "                (mlp): Idefics3VisionMLP(\n",
       "                  (activation_fn): PytorchGELUTanh()\n",
       "                  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (post_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (connector): Idefics3Connector(\n",
       "          (modality_projection): Idefics3SimpleMLP(\n",
       "            (proj): Linear(in_features=12288, out_features=576, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (text_model): LlamaModel(\n",
       "          (embed_tokens): Embedding(49280, 576, padding_idx=2)\n",
       "          (layers): ModuleList(\n",
       "            (0-29): 30 x LlamaDecoderLayer(\n",
       "              (self_attn): LlamaAttention(\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=576, out_features=576, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=576, out_features=32, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=32, out_features=576, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=576, out_features=192, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=576, out_features=32, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=32, out_features=192, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=576, out_features=192, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=576, out_features=32, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=32, out_features=192, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=576, out_features=576, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=576, out_features=32, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=32, out_features=576, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "              (mlp): LlamaMLP(\n",
       "                (gate_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=576, out_features=1536, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=576, out_features=32, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=32, out_features=1536, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (up_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=576, out_features=1536, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=576, out_features=32, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=32, out_features=1536, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (down_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1536, out_features=576, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1536, out_features=32, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=32, out_features=576, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "              (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "      )\n",
       "      (linear): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=576, out_features=128, bias=True)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=576, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for param in model.base_model.model.linear.parameters():\n",
    "    # print(param.dtype)\n",
    "    print(param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=no,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=None,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=trainer_output/runs/Mar30_16-32-36_Vijays-MacBook-Pro.local,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "optim=paged_adamw_8bit,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=trainer_output,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=2,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=trainer_output,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tp_size=0,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.01,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "        output_dir = None,\n",
    "        num_train_epochs = 1,\n",
    "        # max_steps = 10,\n",
    "        per_device_train_batch_size=2,\n",
    "        per_device_eval_batch_size=1,\n",
    "        weight_decay=0.01,\n",
    "        fp16=True, \n",
    "        optim = \"paged_adamw_8bit\"\n",
    "    )\n",
    "\n",
    "print(training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 1442 examples [00:01, 1063.87 examples/s]\n",
      "Generating test split: 500 examples [00:00, 1203.03 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "base_url = \"https://huggingface.co/datasets/vidore/colpali_train_set/resolve/main/data/\"\n",
    "data_files = {\"train\": base_url + \"train-00005-of-00082.parquet\", \"test\": base_url + \"test-00000-of-00001.parquet\"}\n",
    "\n",
    "# Load only the first 1,000 rows of the training split\n",
    "train_subset = load_dataset(\"parquet\", data_files=data_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'image_filename', 'query', 'answer', 'source', 'options', 'page', 'model', 'prompt', 'answer_type'],\n",
       "        num_rows: 1442\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'image_filename', 'query', 'answer', 'source', 'options', 'page', 'model', 'prompt', 'answer_type'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colpali",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
